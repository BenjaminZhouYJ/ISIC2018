\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
%\usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
\usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[pdftex]{graphicx}

\bibliographystyle{abbrvnat}

\title{Lesion Attributes Segmentation for Melanoma Detection with Deep Learning}


\author{
  Eric Z. Chen \thanks{To whom correspondence should be addressed}\\
  Dana-Farber Cancer Institute\\
  Boston, MA, USA \\
  \texttt{zhang\_chen@dfci.harvard.edu} \\
  \And
  Xu Dong \\
  Virginia Tech \\
  Blacksburg, VA, USA \\
  \texttt{xu14@vt.edu} \\
  \AND
  Junyan Wu \\
  Cleerly Inc \\
  New York City, New York, USA \\
  \texttt{mylotarg1989@gmail.com} \\
  \And
  Hongda Jiang \\
  East China University of Science and Technology \\
  Shang Hai, China \\
  \texttt{814473689@qq.com} \\
  \And
  Xiaoxiao Li \\
  Yale University \\
  New Haven, CT, USA \\
  \texttt{xiaoxiao.li@yale.edu} \\
  \And
  Ruichen Rong \\
  UT Southwestern Medical Center \\
  Dallas, TX, USA \\
  \texttt{ruichenrong@gmail.com} \\
}

\begin{document}
% \nipsfinalcopy is no longer used


\maketitle

\begin{abstract}
Melanoma is the most deadly form of skin cancer worldwide. Many efforts have been made for early detection of melanoma. The International Skin Imaging Collaboration (ISIC) hosted the 2018 Challenges to help the diagnosis of melanoma based on dermoscopic images. In this paper, we describe our solutions for the task 2 of ISIC 2018 Challenges. We present two deep learning approaches to automatically detect lesion attributes of melanoma, one is a multi-task U-Net model and the other is a Mask R-CNN based model. Our multi-task U-Net model achieved a Jaccard index of 0.433 on official test data, which ranks the 5th place on the final leaderboard. The code for our solutions is publicly available. 
\end{abstract}


\section{Introduction}



%% skin cancer
Skin cancer is one of the most common cancers worldwide and more than one million skin cancers have been diagnosed in the United States each year. Melanoma is the most dangerous form of skin cancer, which causes over 9,000 deaths each year \citep{ucsw2013united}. Melanoma in the late stage can often spread to other parts of the body and thus it is difficult to treat and usually can be fatal. However, melanoma in the early stage is treatable and the majority can be cured. Many efforts have been made to detect melanoma in the early stage. Dermoscopy is one noninvasive method commonly used in the healthcare to examine pigmented skin lesion. It can generate high-resolution images of the lesion regions on the skin. To diagnose melanoma, it still requires the dermatologist to evaluate the images based on several skin lesion patterns. The common dermoscopic attributes are pigment network, negative network, streaks, milia-like cysts, and globules \citep{mishra2016overview}. Automatic detection of those skin lesion attributes can be a tremendous help for early melanoma diagnosis. 

Towards this goal, the International Skin Imaging Collaboration (ISIC) hosted the 2018 Challenges to diagnose melanoma automatically based on dermoscopic images. The 2018 challenges include three tasks. The first task is to predict lesion segmentation boundaries in dermoscopic images. The second task is to predict the locations of five skin lesion patterns (i.e., dermoscopic attributes) in dermoscopic images, which are pigment network, negative network, streaks, milia-like cysts, and globules. The third task is to predict seven disease categories in dermoscopic images, which includes melanoma, melanocytic nevus, basal cell carcinoma, actinic keratosis / Bowenâ€™s disease, benign keratosis, dermatofibroma and vascular lesion. The three tasks mimic the steps for lesion analysis performance by dermatologists in the clinic settings.  From the previous years' competitions, it has been reported that far less participation in the second task of lesion pattern prediction than in other tasks \citep{codella2018skin}. It seems that the second task is the most difficult one and this motivated us to focus on this problem instead of the other two. 

%% deep learning, segmentation
Deep learning, especially convolutional neural network (CNN), has been widely applied to solve many problems in computer vision. Various CNN based models have been developed for object classification and detection, such as VGG \citep{simonyan2014very}, ResNet \citep{he2016deep}, Inception \citep{szegedy2017inception}, DenseNet \citep{huang2017densely}, and more \citep{liu2017survey}. For image segmentation, FCN \citep{long2015fully}, U-Net \citep{ronneberger2015u}, Mask R-CNN \citep{he2017mask} are some of the most used deep learning models.  Many deep learning based approaches have been applied for melanoma detection \citep{mishra2016overview}. However, most of those researches applied deep learning models to either classify skin diseases \citep{haenssle2018man, yu2018acral} or segment whole lesion regions \citep{zhang2017melanoma}, which are corresponding to the task 1 and task 3 in ISIC 2018 Challenges. To our knowledge, currently there is no deep learning based approach has been applied to segment specific lesion attributes as in the task 2 of ISIC 2018 Challenges. 

%% our methods
In this paper, we describe our solutions for the task 2 of ISIC 2018 Challenges. We present two deep learning approaches to automatically detect lesion attributes of melanoma. One is a U-Net based approach and the other is a Mask R-CNN based approach. In the U-Net model, we replaced the encoder part of the U-Net with a pretrained VGG16 network \citep{shvets2018automatic}. In the middle layer and the last layer of the U-Net, we added two classification heads to classify the empty masks versus the non-empty masks.  Our multi-task U-Net model achieved a Jaccard index of 0.433 on official test data, which ranks the 5th place on the final leaderboard. The code for our solutions is publicly available at https://github.com/chvlyl/ISIC2018. 

\section{Methods}


\subsection{Data}
We downloaded the task 2 dataset from ISIC 2018 Challenges website \citep{Tschandl2018_HAM10000}. For each skin image, this task is aimed at detecting five lesion attributes (pigment network, negative network, streaks, milia-like cysts, and globules). The dataset consists of 2,594 images and 12,970 ground truth masks (five masks for each skin image) as training data. The skin images are RGB images in JPG format and the masks are grayscale images in PNG format. The five masks are corresponding to five lesion attributes in each skin image. Note that some of the attributes may not present in skin images, therefore the corresponding masks are empty. There are 100 images provided as validation data and 1,000 images as test data.  No ground truth masks are provided for validation data and test data during the competition. 

\subsection{U-Net based approach}
Based on the original U-Net \citep{ronneberger2015u}, we replace the encoder part with a pretrained VGG16 \citep{iglovikov2018ternausnetv2}. The pretrained weights are from ImageNet. In the skip connections, the feature maps from the encoder part are concatenated with the corresponding feature maps in the decoder part. The output of the U-Net is a five channel mask. We define the loss function for pixel wise segmentation as a combination of binary cross entropy loss L and Jaccard index J \citep{shvets2018automatic}. 
\begin{equation}
loss_1 = L - log J
\end{equation}

We observed that for each skin images, not all lesion attributes present in the masks. Therefore, we also add two classification heads to classify the empty masks versus the non-empty masks. Since the last layer of our U-Net is a convolutional layer and the output is five feature maps, we add a global max pooling layer to this last convolutional layer and use this to classify the empty and non-empty masks for five attributes.  This classification head will act as a regularizer. Since if the truth mask is empty and our network predicts a non-empty mask, after global max pooling, the classification loss will be large. Therefore, this classification head will force the network to predict an empty mask if the true mask is empty.  We also adapt the similar idea in Inception \citep{szegedy2017inception} and add another classification head to the middle layer with a $1\times1$ convolutional layer and a global average pooling layer. The network structure is shown in Figure 1.

The final loss is a combination of pixel wise segmentation loss $loss_1$, classification loss using the middle layer $loss_2$ and classification loss using the last layer $loss_3$. We used binary cross entropy for $loss_2$ and $loss_3$. We assigned weights $0.5$ to the last two losses.  
\begin{equation}
loss=loss_1+0.5\times loss_2 + 0.5\times loss_3
\end{equation}

The raw skin images have various sizes, from $540\times722$ to $4499\times6748$. We first resize all the skin images and corresponding masks to $512\times512$. Then for each skin image, we concatenate the five corresponding grayscale masks into a five-channel mask. Each channel is a $512\times512$ mask.  Both skin images and mask images are divided by 255 to scale the values in [0,1].

We first freeze all the encoder layers and only train the decoder layers for 50 epochs. Then we train the whole network for another 250 epochs. We use a learning rate of 0.01 with Adam optimizer and then decrease it by 0.8 when validation loss does not improve for 5 epochs during training. During training, we augment the skin images and masks by random flipping, rotation, scaling as well as randomly adjusting brightness and saturation. We use cutoff 0.3 to transform the predicted probability into the binary mask. The whole analysis is implemented in Pytorch. 

\begin{figure}
\centering
\label{fig1}
\includegraphics[scale=0.4,trim=0 200 0 0]{vgg.pdf}
\caption{Network structure using in U-Net based approach. The encoder part of the U-Net is replaced with a pretrained VGG16 network \citep{shvets2018automatic}. We also add two classification heads to the network, where one is added to the middle layer with $1\times1$ convolution and global average pooling layers and the other is added to the last layer with a global max pooling layer.}
\end{figure}

\subsection{Mask R-CNN based approach}
%% Xu Dong
Mask R-CNN has been recognized as the state-of-art neutral network architecture for the instance segmentation. It utilizes the region based convolution neutral network and achieves final pixel-level segmentation based on the generated bounding box. The success of Mask R-CNN in the medical image field has been demonstrated by some previous work. Here we adapted the Mask R-CNN architecture for the skin attribute detection task in the challenge.

We used the ResNet 101 as the backbone of the network. The backbone weights are initialized by the weights trained in the coco dataset. We added the head layers onto the ResNet 101 and the gradient backpropagation was only done to the head layer during the training. The Adam optimizer was used with the initial learning rate as 0.001 and momentum as 0.99.

We did the training directly using the original images in the provided dataset without any preprocessing because it gave better result compared with training onto the resized images (1024 by 1024). To promote the training, only those image with non-empty attribute masks are used in the training. Under such training strategy, the best Jaccard index we obtained is 0.233.

\section{Results}
Figure 2 shows an example of several skin images and their corresponding lesion attributes (for each attribute, top rows show the ground truth masks).  We noticed that not all attributes present in each skin image. That is, some of the masks are empty for certain skin images. Table 1 shows the summary of non-empty masks for each lesion attribute. 58.7\%, 26.3\% and 23.2\% of the skin images have non-empty masks for Pigment network, milia-like cysts, and globules, respectively.  While only 7.3\% and 2.9\% of the skin images have non-empty masks for negative network and streaks, respectively. Since segmentation is essentially a pixel-wise classification problem, a large number of empty masks increase the number of samples in negative class. Therefore, in the Mask R-CNN based approach, we train the model only using the non-empty masks. In the U-Net based approach, we add two classification heads to classify the empty versus non-empty masks. Figure 2 shows the predicted masks from the U-Net based approach (for each attribute, the bottom rows show the predicted masks). The Jaccard index on the official validation dataset is 0.477 for U-Net based approach and 0.233 for Mask R-CNN based approach (Table 2). 

%\begin{figure}
%  \centering
%  \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%  \caption{Sample figure caption.}
%  \includegraphics{skin1.png}
%\end{figure}

%\begin{verbatim}
%\usepackage[pdftex]{graphicx}
%\end{verbatim}



\begin{figure}
\centering
\label{fig2}
\includegraphics[scale=0.65,trim=0 0 0 0]{skin.png}
\caption{Example of skin images and five corresponding lesion attributes. For each attribute, the top row is the ground truth masks and the bottom row is the predicted masks from U-Net based approach.}
\end{figure}


\begin{table}
  \caption{Summary of non-empty masks in the training data}
  \label{summary-training}
  \centering
  \begin{tabular}{lcc}
    \toprule
    %\multicolumn{2}{c}{Part}                   \\
    %\cmidrule(r){1-2}
    Attributes     &   Number of non-empty masks   & Percentage   \\
    \midrule
    Pigment network & 1522  & 58.7\%    \\
    Negative network     & 189 & 7.3\%      \\
    Streaks     & 100       & 2.9\%  \\
    Milia-like cysts & 681  & 26.3\%     \\
    Globules     & 602 & 23.2\%      \\
    \midrule
    Total skin images & 2594 & 100\% \\
    \bottomrule
  \end{tabular}
\end{table}


\begin{table}
  \caption{Jaccard index on the official validation and test dataset}
  \label{sample-table}
  \centering
  \begin{tabular}{lcc}
    \toprule
    %\multicolumn{3}{c}{Part}                   \\
    %\cmidrule(r){1-3}
    Model     & Validation & Test   \\
    \midrule
    U-Net & 0.477   & 0.433  \\ 
    Mask R-CNN     & 0.233  & NA     \\
    \bottomrule
  \end{tabular}
\end{table}


\subsection{Discussion}
In this paper, we proposed two deep learning based approaches to detect five attributes of skin lesions. We achieved a Jaccard index of 0.477 and 0.233 on the official validation data. The final Jaccard index on the official test data is 0.433 for our multi-task U-Net model, which ranks the 5th place on the final leaderboard.  We didn't submit our test result for the Mask R-CNN model.

For our U-Net based model, we tried to add the batch normalize layers to the encoder and decoder parts of the network. However, this does not improve the model performance compared to the model without batch normalization layers. We also tried to replace the encoder part of the U-Net network with pretrained DenseNet. However, the GPU memory usage was too hight and we could only fit one image per patch. The training took a long time and the final results were worse than VGG as the encoder. 

We also noticed that the five lesion attributes are highly imbalanced in the training data. For example, 58.7\% of the skin images have the pigment network attribute but only 2.9\% of skin images have the negative network attribute. Classification of imbalanced data is often difficult. One potential solution is to assign different weighs to each attribute during training. However, we spent some time exploring different weight configurations but it seems to us there was no significant improvement. 



%-------------------------------------------------%
%-------------------------------------------------%
%-------------------------------------------------%
%-------------------------------------------------%


\bibliography{ref}



\end{document}
