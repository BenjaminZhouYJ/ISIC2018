% Template for ISBI-2018 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{Lesion Attributes Segmentation for Melanoma Detection with Deep Learning}
%
% Single address.
% ---------------
%\name{Author(s) Name(s)\thanks{Thanks to XYZ agency for funding.}}
%\address{Author Affiliation(s)}
%
% For example:
% ------------
%\address{School\\
%    Department\\
%    Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%    {School A-B\\
%    Department A-B\\
%    Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%    while at ...}}
%    {School C-D\\
%    Department C-D\\
%    Address C-D}
%
% More than two addresses
% -----------------------
 \name{
 Eric Z. Chen$^{1}\sthanks{To whom correspondence should be addressed}$ 
 \qquad Xu Dong$^{2}$
 \qquad Junyan Wu$^{3}$ 
 \qquad Hongda Jiang$^{4}$ 
 \qquad Xiaoxiao Li$^{5}$
 \qquad Ruichen Rong$^{6}$
 }

\address{
$^{1}$ Dana-Farber Cancer Institute, Boston, MA, USA \\
$^{2}$Virginia Tech, Blacksburg, VA, USA \\
$^{3}$Cleerly Inc, New York City, New York, USA \\
$^{4}$East China University of Science and Technology, Shang Hai, China \\
$^{5}$Yale University, New Haven, CT, USA\\
$^{6}$UT Southwestern Medical Center, Dallas, TX, USA
}



\begin{document}
%\ninept
%
\maketitle
%
\begin{abstract}
Melanoma is the most deadly form of skin cancer worldwide. Many efforts have been made for early detection of melanoma. The International Skin Imaging Collaboration (ISIC) hosted the 2018 Challenges to improve the diagnosis of melanoma based on dermoscopic images. In this paper, we describe our solution for the task 2 of ISIC 2018 Challenges. We present a multi-task U-Net model  to automatically detect lesion attributes of melanoma. Our multi-task U-Net deep learning model achieves a Jaccard index of 0.433 on official test data, which ranks the 5th place on the final leaderboard. 
\end{abstract}
%
\begin{keywords}
Skin cancer, deep learning
\end{keywords}
%
\section{Introduction}
\label{sec:intro}

%% skin cancer
Skin cancer is one of the most common cancers worldwide and more than one million skin cancers have been diagnosed in the United States each year. Melanoma is the most dangerous form of skin cancer, which causes over 9,000 deaths each year \cite{ucsw2013united}. Melanoma in the late stage can often spread to other parts of the body and thus it is difficult to treat and usually can be fatal. However, melanoma in the early stage is treatable and the majority can be cured. Many efforts have been made to detect melanoma in the early stage. Dermoscopy is one noninvasive method commonly used in the healthcare to examine pigmented skin lesion. It can generate high-resolution images of the lesion regions on the skin. To diagnose melanoma, it still requires the dermatologist to evaluate the images based on several skin lesion patterns. The common dermoscopic attributes are pigment network, negative network, streaks, milia-like cysts, and globules \cite{mishra2016overview}. Automatic detection of those skin lesion attributes can be a tremendous help for early melanoma diagnosis. 

Towards this goal, the International Skin Imaging Collaboration (ISIC) hosted the 2018 Challenges to diagnose melanoma automatically based on dermoscopic images. The 2018 challenges include three tasks. The first task is to predict lesion segmentation boundaries in dermoscopic images. The second task is to predict the locations of five skin lesion patterns (i.e., dermoscopic attributes) in dermoscopic images, which are pigment network, negative network, streaks, milia-like cysts, and globules. The third task is to predict seven disease categories in dermoscopic images, which includes melanoma, melanocytic nevus, basal cell carcinoma, actinic keratosis / Bowenâ€™s disease, benign keratosis, dermatofibroma and vascular lesion. The three tasks mimic the steps for lesion analysis performed by dermatologists in the clinic settings.  From the previous years' competitions, it has been reported that far less participation in the second task of lesion pattern prediction than in other tasks \cite{codella2018skin}. It seems that the second task is the most difficult one and this motivated us to focus on this problem instead of the other two. 

%% deep learning, segmentation
Deep learning, especially convolutional neural network (CNN), has been widely applied to solve many problems in computer vision. Various CNN based models have been developed for object classification and detection, such as VGG \cite{simonyan2014very}, ResNet \cite{he2016deep}, Inception \cite{szegedy2017inception}, DenseNet \cite{huang2017densely}, and more \cite{liu2017survey}. For image segmentation, FCN \cite{long2015fully}, U-Net \cite{ronneberger2015u}, Mask R-CNN \cite{he2017mask} are some of the popular deep learning models.  Many deep learning based approaches have been applied for melanoma detection \cite{mishra2016overview}. However, most of those researches applied deep learning models to either classify skin diseases \cite{haenssle2018man, yu2018acral} or segment whole lesion regions \cite{zhang2017melanoma}, which are corresponding to the task 1 and task 3 in ISIC 2018 Challenges. To our knowledge, currently there is no deep learning based approach has been applied to segment specific lesion attributes as in the task 2 of ISIC 2018 Challenges. Note that in the previous ISIC Challenges, task 2 was based on the segmentation of superpixels \cite{codella2018skin}.   

%% our methods
In this paper, we describe our solution for the task 2 of ISIC 2018 Challenges. We present a multi-task U-Net model to automatically detect lesion attributes of melanoma. In the U-Net model, we replace the encoder part of the U-Net with a pretrained VGG16 network \cite{shvets2018automatic}. In the middle layer and the last layer of the U-Net, we add two classification heads to classify the empty masks versus the non-empty masks.  Our multi-task U-Net model achieves a Jaccard index of 0.433 on official test data, which ranks the 5th place on the final leaderboard. The code for our solutions is publicly available at https://github.com/chvlyl/ISIC2018. 

\begin{figure}[t]
\begin{minipage}[!ht]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[scale=0.45,trim=0 20 0 20]{skin.png}}
%  \vspace{2.0cm}
%  \centerline{(a) Result 1}\medskip
\end{minipage}
\caption{Example of skin images and five corresponding lesion attributes. For each attribute, the top row is the ground truth masks and the bottom row is the predicted masks from multi-task U-Net model.}
\label{fig:res}
\end{figure}

\begin{figure*}[!ht]
\begin{center}
\includegraphics[scale=0.5,trim=0 200 0 0]{vgg.pdf}
\end{center}
   \caption{Network structure using U-Net architecture. The encoder part of the U-Net is replaced with a pretrained VGG16 network \cite{shvets2018automatic}. We also add two classification heads to the network, where one is added to the middle layer with $1\times1$ convolution and global average pooling layers and the other is added to the last layer with a global max pooling layer.}
\label{fig:short}
\end{figure*}


\section{Methods}


\subsection{Data}
We downloaded the task 2 dataset from ISIC 2018 Challenges website \cite{Tschandl2018_HAM10000}. For each skin image, this task is aimed at detecting five lesion attributes (pigment network, negative network, streaks, milia-like cysts, and globules). The dataset consists of 2,594 images and 12,970 ground truth masks (five masks for each skin image) as training data. The skin images are RGB images in JPG format and the masks are grayscale images in PNG format. The five masks are corresponding to five lesion attributes in each skin image. Note that some of the attributes may not present in skin images, therefore the corresponding masks are empty. There are 100 images provided as validation data and 1,000 images as test data.  No ground truth masks are provided for validation data and test data during the competition. 

We noticed that not all attributes present in each skin image (Figure 1), which motivated us to propose the following multi-task U-Net model.

\subsection{Multi-task U-Net model}
Based on the original U-Net \cite{ronneberger2015u}, we replace the encoder part with a pretrained VGG16 \cite{iglovikov2018ternausnetv2}. The pretrained weights are based on ImageNet data. In the skip connections, the feature maps from the encoder part are concatenated with the corresponding feature maps in the decoder part. The output of the U-Net is a five channel mask. We define the loss function for pixel wise segmentation as a combination of binary cross entropy loss L and Jaccard index J \cite{shvets2018automatic}. 
\begin{equation}
loss_1 = L - log J
\end{equation}

We observed that for each skin images, not all lesion attributes present in the masks. Therefore, we also add two classification heads to classify the empty masks versus the non-empty masks. Since the last layer of our U-Net is a convolutional layer and the output is five feature maps, we add a global max pooling layer to this last convolutional layer and use this to classify the empty and non-empty masks for five attributes.  This classification head will act as a regularizer. Since if the truth mask is empty and our network predicts a non-empty mask, after global max pooling, the classification loss will be large. Therefore, this classification head will force the network to predict an empty mask if the true mask is empty.  We also adopt the similar idea in Inception \cite{szegedy2017inception} and add another classification head to the middle layer with a $1\times1$ convolutional layer and a global average pooling layer. The network structure is shown in Figure 2.

The final loss is a combination of pixel wise segmentation loss $loss_1$, classification loss using the middle layer $loss_2$ and classification loss using the last layer $loss_3$. We used binary cross entropy for $loss_2$ and $loss_3$. We assigned weights $0.5$ to the last two losses.  
\begin{equation}
loss=loss_1+0.5\times loss_2 + 0.5\times loss_3
\end{equation}

\subsection{Model training}
The raw skin images have various sizes, from $540\times722$ to $4499\times6748$. We first resize all the skin images and corresponding masks to $512\times512$. Then for each skin image, we concatenate the five corresponding grayscale masks into a five-channel mask. Each channel is a $512\times512$ mask.  Both skin images and mask images are divided by 255 to scale the values in [0,1].

We first freeze all the encoder layers and only train the decoder layers for 50 epochs. Then we train the whole network for another 250 epochs. We use a learning rate of 0.01 with Adam optimizer and then decrease it by 0.8 when validation loss does not improve for 5 epochs during training. During training, we augment the skin images and masks by random flipping, rotation, scaling as well as randomly adjusting brightness and saturation. We use cutoff 0.3 to transform the predicted probability into the binary mask. The whole analysis is implemented in Pytorch. 


\subsection{Results}
Figure 1 shows an example of several skin images and their corresponding lesion attributes (for each attribute, top rows show the ground truth masks).  We noticed that not all attributes present in each skin image. That is, some of the masks are empty for certain skin images. Table 1 shows the summary of non-empty masks for each lesion attribute. 58.7\%, 26.3\% and 23.2\% of the skin images have non-empty masks for Pigment network, milia-like cysts, and globules, respectively.  While only 7.3\% and 2.9\% of the skin images have non-empty masks for negative network and streaks, respectively. Since segmentation is essentially a pixel-wise classification problem, a large number of empty masks increase the number of samples in negative class.  In the U-Net model, we add two classification heads to classify the empty versus non-empty masks. Figure 1 shows the predicted masks from the multi-task U-Net model (for each attribute, the bottom rows show the predicted masks). The Jaccard index is 0.477 on the official validation dataset. The final Jaccard index on the official test data is 0.433, which ranks the 5th place on the final leaderboard.

%\begin{figure}
%  \centering
%  \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%  \caption{Sample figure caption.}
%  \includegraphics{skin1.png}
%\end{figure}

%\begin{verbatim}
%\usepackage[pdftex]{graphicx}
%\end{verbatim}





% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
%\vfill
%\pagebreak


\begin{table}
  \caption{Summary of non-empty masks in the training data}
  \label{summary-training}
  \centering
  \begin{tabular}{lcc}
    \toprule
    %\multicolumn{2}{c}{Part}                   \\
    %\cmidrule(r){1-2}
    Attributes     &   Number of non-empty masks   & Percentage   \\
    \midrule
    Pigment network & 1522  & 58.7\%    \\
    Negative network     & 189 & 7.3\%      \\
    Streaks     & 100       & 2.9\%  \\
    Milia-like cysts & 681  & 26.3\%     \\
    Globules     & 602 & 23.2\%      \\
    \midrule
    Total skin images & 2594 & 100\% \\
    \bottomrule
  \end{tabular}
\end{table}


%\begin{table}
%  \caption{Jaccard index on the official validation and test dataset}
%  \label{sample-table}
%  \centering
%  \begin{tabular}{lcc}
%    \toprule
%    %\multicolumn{3}{c}{Part}                   \\
%    %\cmidrule(r){1-3}
%    Model     & Validation & Test   \\
%    \midrule
%    U-Net & 0.477   & 0.433  \\ 
%    Mask R-CNN     & 0.233  & NA     \\
%    \bottomrule
%  \end{tabular}
%\end{table}


\subsection{Discussion}
In this paper, we propose a multi-task U-Net model to detect five attributes of skin lesions. We replace the encoder part of the U-Net with a pretrained VGG16 network and add two classification heads to the U-Net. Our submission ranks the 5th place on the final leaderboard of ISIC 2018 task 2. 

For our U-Net based model, we tried to add the batch normalize layers to the encoder and decoder parts of the network. However, this does not improve the model performance compared to the model without batch normalization layers. We also tried to replace the encoder part of the U-Net network with pretrained DenseNet. However, the GPU memory usage was too hight and we could only fit one image per patch. The training took a long time and the final result was worse than that with VGG16 as the encoder. 

We also noticed that the five lesion attributes are highly imbalanced in the training data. For example, 58.7\% of the skin images have the pigment network attribute but only 2.9\% of skin images have the negative network attribute. Classification of imbalanced data is often difficult. One potential solution is to assign different weighs to each attribute during training. However, we spent some time exploring different weight configurations but it seems to us there was no significant improvement. 

Mask R-CNN has been recognized as the state-of-art neutral network architecture for instance segmentation.  We also applied the Mask R-CNN architecture for the skin attribute detection task in the challenge. The best Jaccard index on the official validation data is 0.233 and we did not submit our test result for the Mask R-CNN model. 

% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{ref}

\end{document}
